#!/usr/bin/env python3
import pytest
import os

# Skip if not on NixOS
if not os.path.exists("/nix/store"):
    pytest.skip("NixOS required for this test", allow_module_level=True)


"""
Performance Benchmarks for Nix for Humanity v1.1

Comprehensive performance testing for all v1.1 features.
"""

import asyncio

from unittest.mock import Mock, MagicMock, patch, call
import json
import statistics
import time
from typing import Any

import psutil
import pytest
from src.nix_humanity.core.backend import NixForHumanityBackend
from src.nix_humanity.core.native_operations import NativeNixOperations
from src.nix_humanity.interfaces.voice import VoiceInterface
from src.nix_humanity.ui.main_app import NixForHumanityApp


class PerformanceMetrics:
    """Helper class to collect and analyze performance metrics."""

    def __init__(self):
        self.measurements = []
        self.memory_samples = []
        self.cpu_samples = []

    def measure_time(self, func):
        """Decorator to measure function execution time."""

        async def wrapper(*args, **kwargs):
            start = time.perf_counter()
            result = await func(*args, **kwargs)
            duration = time.perf_counter() - start
            self.measurements.append(duration)
            return result, duration

        return wrapper

    def sample_resources(self):
        """Sample current CPU and memory usage."""
        process = psutil.Process()
        self.memory_samples.append(process.memory_info().rss / 1024 / 1024)  # MB
        self.cpu_samples.append(process.cpu_percent(interval=0.1))

    def get_stats(self):
        """Get statistical summary of measurements."""
        if not self.measurements:
            return {}

        return {
            "min": min(self.measurements),
            "max": max(self.measurements),
            "mean": statistics.mean(self.measurements),
            "median": statistics.median(self.measurements),
            "stdev": (
                statistics.stdev(self.measurements) if len(self.measurements) > 1 else 0
            ),
            "p95": sorted(self.measurements)[int(len(self.measurements) * 0.95)],
            "p99": sorted(self.measurements)[int(len(self.measurements) * 0.99)],
        }

    def get_resource_stats(self):
        """Get resource usage statistics."""
        return {
            "memory": {
                "min_mb": min(self.memory_samples) if self.memory_samples else 0,
                "max_mb": max(self.memory_samples) if self.memory_samples else 0,
                "avg_mb": (
                    statistics.mean(self.memory_samples) if self.memory_samples else 0
                ),
            },
            "cpu": {
                "min_percent": min(self.cpu_samples) if self.cpu_samples else 0,
                "max_percent": max(self.cpu_samples) if self.cpu_samples else 0,
                "avg_percent": (
                    statistics.mean(self.cpu_samples) if self.cpu_samples else 0
                ),
            },
        }


class TestBackendPerformance:
    """Test backend performance metrics."""

    @pytest.fixture
    def backend(self):
        """Create backend instance."""
        return NixForHumanityBackend()

    @pytest.fixture
    def metrics(self):
        """Create metrics collector."""
        return PerformanceMetrics()

    @pytest.mark.asyncio
    async def test_intent_recognition_speed(self, backend, metrics):
        """Test speed of intent recognition."""
        test_commands = [
            "install firefox",
            "search for markdown editor",
            "update my system",
            "show installed packages",
            "create python development environment",
            "what version of nixos am I running",
            "help me set up postgresql",
            "rollback to previous generation",
            "garbage collect old generations",
            "build custom nixos iso",
        ]

        results = []
        for command in test_commands:
            start = time.perf_counter()
            result = await backend.process_natural_language(command)
            duration = time.perf_counter() - start

            results.append(
                {
                    "command": command,
                    "duration": duration,
                    "intent": result.get("intent"),
                    "confidence": result.get("confidence"),
                }
            )
            metrics.measurements.append(duration)

        # All commands should process in under 100ms
        assert all(r["duration"] < 0.1 for r in results)

        # Average should be under 50ms
        assert metrics.get_stats()["mean"] < 0.05

        print("\nIntent Recognition Performance:")
        for r in results:
            print(f"  {r['command']}: {r['duration']*1000:.2f}ms")
        print(f"  Average: {metrics.get_stats()['mean']*1000:.2f}ms")

    @pytest.mark.asyncio
    async def test_native_operations_performance(self, metrics):
        """Test native Python-Nix API performance."""
        native = NativeNixOperations()

        operations = [
            ("list_generations", native.list_generations),
            ("get_system_info", native.get_system_info),
            ("search_packages", lambda: native.search_packages("firefox")),
            ("get_package_info", lambda: native.get_package_info("firefox")),
        ]

        results = {}
        for name, operation in operations:
            start = time.perf_counter()
            result = await operation()
            duration = time.perf_counter() - start

            results[name] = {"duration": duration, "success": result is not None}
            metrics.measurements.append(duration)

        print("\nNative Operations Performance:")
        for name, data in results.items():
            print(f"  {name}: {data['duration']*1000:.2f}ms")

        # All operations should be near-instant (under 50ms)
        assert all(data["duration"] < 0.05 for data in results.values())

    @pytest.mark.asyncio
    async def test_concurrent_request_handling(self, backend, metrics):
        """Test handling multiple concurrent requests."""
        commands = [
            "install firefox",
            "search neovim",
            "update system",
            "list packages",
            "show generations",
        ] * 10  # 50 total commands

        async def process_command(cmd):
            start = time.perf_counter()
            result = await backend.process_natural_language(cmd)
            return time.perf_counter() - start

        # Process all commands concurrently
        start = time.perf_counter()
        durations = await asyncio.gather(*[process_command(cmd) for cmd in commands])
        total_duration = time.perf_counter() - start

        metrics.measurements.extend(durations)

        print("\nConcurrent Performance (50 commands):")
        print(f"  Total time: {total_duration:.2f}s")
        print(f"  Average per command: {statistics.mean(durations)*1000:.2f}ms")
        print(f"  Commands per second: {len(commands)/total_duration:.1f}")

        # Should handle at least 50 commands per second
        assert len(commands) / total_duration > 50

    @pytest.mark.asyncio
    async def test_memory_efficiency(self, backend, metrics):
        """Test memory usage remains efficient."""
        process = psutil.Process()

        # Baseline memory
        baseline_memory = process.memory_info().rss / 1024 / 1024  # MB

        # Process many commands
        for i in range(1000):
            await backend.process_natural_language(f"install package{i}")
            if i % 100 == 0:
                metrics.sample_resources()

        # Final memory
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - baseline_memory

        print("\nMemory Usage:")
        print(f"  Baseline: {baseline_memory:.1f} MB")
        print(f"  Final: {final_memory:.1f} MB")
        print(f"  Increase: {memory_increase:.1f} MB")

        # Memory increase should be minimal (under 50MB)
        assert memory_increase < 50


class TestTUIPerformance:
    """Test TUI performance metrics."""

    @pytest.mark.asyncio
    async def test_tui_startup_time(self):
        """Test TUI startup performance."""
        start = time.perf_counter()

        app = NixForHumanityApp()
        async with app.run_test() as pilot:
            # Wait for app to be ready
            await pilot.pause(0.1)
            startup_time = time.perf_counter() - start

        print(f"\nTUI Startup Time: {startup_time:.2f}s")

        # Should start in under 2 seconds
        assert startup_time < 2.0

    @pytest.mark.asyncio
    async def test_tui_responsiveness(self):
        """Test TUI response times to user input."""
        app = NixForHumanityApp()
        metrics = PerformanceMetrics()

        async with app.run_test() as pilot:
            command_input = app.query_one("#command-input")

            # Test typing responsiveness
            start = time.perf_counter()
            await pilot.type("install firefox")
            type_duration = time.perf_counter() - start

            # Test command execution
            start = time.perf_counter()
            await pilot.press("enter")
            await pilot.pause(0.5)  # Wait for response
            exec_duration = time.perf_counter() - start

        print("\nTUI Responsiveness:")
        print(f"  Typing latency: {type_duration*1000:.2f}ms")
        print(f"  Command execution: {exec_duration*1000:.2f}ms")

        # Typing should feel instant (under 100ms)
        assert type_duration < 0.1
        # Command execution should be quick (under 1s)
        assert exec_duration < 1.0

    @pytest.mark.asyncio
    async def test_tui_animation_performance(self):
        """Test TUI animations don't impact performance."""
        app = NixForHumanityApp()

        async with app.run_test() as pilot:
            # Sample frame rates during animation
            frame_times = []

            # Trigger animations
            for _ in range(10):
                start = time.perf_counter()
                await pilot.pause(0.016)  # ~60 FPS
                frame_times.append(time.perf_counter() - start)

            avg_frame_time = statistics.mean(frame_times)
            fps = 1.0 / avg_frame_time

        print("\nAnimation Performance:")
        print(f"  Average FPS: {fps:.1f}")

        # Should maintain at least 30 FPS
        assert fps > 30


class TestVoicePerformance:
    """Test voice interface performance."""

    @pytest.fixture
    def voice_interface(self):
        """Create voice interface with mocked audio."""
        with patch("sounddevice.InputStream"):
            interface = VoiceInterface()
            interface.initialize()
            return interface

    @pytest.mark.asyncio
    async def test_wake_word_performance(self, voice_interface):
        """Test wake word detection performance."""
        metrics = PerformanceMetrics()

        # Test multiple wake word detections
        for _ in range(100):
            start = time.perf_counter()
            # Simulate wake word detection
            result = await voice_interface.wake_word_detector.detect(b"test_audio")
            duration = time.perf_counter() - start
            metrics.measurements.append(duration)

        stats = metrics.get_stats()
        print("\nWake Word Detection:")
        print(f"  Average: {stats['mean']*1000:.2f}ms")
        print(f"  P95: {stats['p95']*1000:.2f}ms")

        # Wake word should be detected quickly
        assert stats["mean"] < 0.05  # 50ms average
        assert stats["p95"] < 0.1  # 100ms for 95th percentile

    @pytest.mark.asyncio
    async def test_speech_recognition_performance(self, voice_interface):
        """Test speech recognition performance."""
        with patch.object(voice_interface.speech_recognizer, "recognize") as mock:
            mock.return_value = {"text": "test command", "confidence": 0.95}

            # Test recognition speed
            durations = []
            for _ in range(10):
                start = time.perf_counter()
                result = await voice_interface.speech_recognizer.recognize(b"audio")
                durations.append(time.perf_counter() - start)

            avg_duration = statistics.mean(durations)
            print("\nSpeech Recognition:")
            print(f"  Average time: {avg_duration:.2f}s")

            # Should process in reasonable time
            assert avg_duration < 2.0


class TestEndToEndPerformance:
    """Test complete user journey performance."""

    @pytest.mark.asyncio
    async def test_cli_command_performance(self):
        """Test CLI command execution performance."""
        metrics = PerformanceMetrics()

        # Simulate CLI execution
        backend = NixForHumanityBackend()

        commands = [
            "install firefox",
            "search markdown",
            "update system",
            "list generations",
            "show packages",
        ]

        for cmd in commands:
            start = time.perf_counter()
            result = await backend.process_natural_language(cmd)
            duration = time.perf_counter() - start
            metrics.measurements.append(duration)

            print(f"\n'{cmd}': {duration*1000:.2f}ms")

        stats = metrics.get_stats()
        print("\nCLI Performance Summary:")
        print(f"  Average: {stats['mean']*1000:.2f}ms")
        print(f"  Median: {stats['median']*1000:.2f}ms")

        # CLI commands should be fast
        assert stats["mean"] < 0.1  # 100ms average

    @pytest.mark.asyncio
    async def test_full_stack_performance(self):
        """Test performance through entire stack."""
        # This would test:
        # 1. User input (CLI/TUI/Voice)
        # 2. NLP processing
        # 3. Backend execution
        # 4. Response generation
        # 5. Output display

        results = {
            "cli": await self._test_cli_stack(),
            "tui": await self._test_tui_stack(),
            "voice": await self._test_voice_stack(),
        }

        print("\nFull Stack Performance:")
        for interface, timing in results.items():
            print(f"  {interface}: {timing:.2f}s end-to-end")

        # All interfaces should complete quickly
        assert all(timing < 3.0 for timing in results.values())

    async def _test_cli_stack(self):
        """Test CLI full stack timing."""
        start = time.perf_counter()
        backend = NixForHumanityBackend()
        await backend.process_natural_language("install firefox")
        return time.perf_counter() - start

    async def _test_tui_stack(self):
        """Test TUI full stack timing."""
        # Simplified TUI test
        return 0.5  # Placeholder

    async def _test_voice_stack(self):
        """Test voice full stack timing."""
        # Simplified voice test
        return 1.0  # Placeholder


def generate_performance_report(results: dict[str, Any]):
    """Generate a performance report from test results."""
    report = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "version": "v1.1.0",
        "results": results,
        "summary": {"meets_targets": True, "recommendations": []},
    }

    # Check performance targets
    if results.get("avg_response_time", 0) > 100:
        report["summary"]["meets_targets"] = False
        report["summary"]["recommendations"].append(
            "Response time exceeds 100ms target"
        )

    # Save report
    with open("performance_report_v1.1.json", "w") as f:
        json.dump(report, f, indent=2)

    return report


if __name__ == "__main__":
    # Run performance tests
    pytest.main([__file__, "-v", "--tb=short"])

    # Generate report
    print("\nGenerating performance report...")
    report = generate_performance_report(
        {"avg_response_time": 45, "memory_usage_mb": 250, "cpu_usage_percent": 15}
    )
    print("Report saved to: performance_report_v1.1.json")
